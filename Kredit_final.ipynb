{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kreditwürdigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#Loading the data and naming the columns\n",
    "def get_data():\n",
    "    df=pd.read_csv('kredit.dat',sep='\\t', header=None, na_values=['?'])\n",
    "    df.columns=[\n",
    "    'account','duration','history','purpose','amount','savings','employment','rate','status_and_sex',\n",
    "    'guarantors','residence','property','age','plans','housing','credits','job','maintenance',\n",
    "    'telephone','foreigner','creditworthy']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account</th>\n",
       "      <th>duration</th>\n",
       "      <th>history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment</th>\n",
       "      <th>rate</th>\n",
       "      <th>status_and_sex</th>\n",
       "      <th>guarantors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>credits</th>\n",
       "      <th>job</th>\n",
       "      <th>maintenance</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreigner</th>\n",
       "      <th>creditworthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A14</td>\n",
       "      <td>36</td>\n",
       "      <td>A32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2299</td>\n",
       "      <td>A63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A123</td>\n",
       "      <td>39</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>18</td>\n",
       "      <td>A32</td>\n",
       "      <td>A46</td>\n",
       "      <td>1239</td>\n",
       "      <td>A65</td>\n",
       "      <td>A73</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>61</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A13</td>\n",
       "      <td>24</td>\n",
       "      <td>A32</td>\n",
       "      <td>A40</td>\n",
       "      <td>947</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>38</td>\n",
       "      <td>A141</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A14</td>\n",
       "      <td>15</td>\n",
       "      <td>A33</td>\n",
       "      <td>A43</td>\n",
       "      <td>1478</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>4</td>\n",
       "      <td>A94</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>33</td>\n",
       "      <td>A141</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A14</td>\n",
       "      <td>24</td>\n",
       "      <td>A32</td>\n",
       "      <td>A40</td>\n",
       "      <td>1525</td>\n",
       "      <td>A64</td>\n",
       "      <td>A74</td>\n",
       "      <td>4</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A123</td>\n",
       "      <td>34</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  account  duration history purpose  amount savings employment  rate  \\\n",
       "0     A14        36     A32     NaN    2299     A63        NaN     4   \n",
       "1     A12        18     A32     A46    1239     A65        A73     4   \n",
       "2     A13        24     A32     A40     947     A61        A74     4   \n",
       "3     A14        15     A33     A43    1478     A61        A73     4   \n",
       "4     A14        24     A32     A40    1525     A64        A74     4   \n",
       "\n",
       "  status_and_sex guarantors     ...       property age  plans housing credits  \\\n",
       "0            A93       A101     ...           A123  39   A143    A152       1   \n",
       "1            A93       A101     ...           A124  61   A143    A153       1   \n",
       "2            A93       A101     ...           A124  38   A141    A153       1   \n",
       "3            A94       A101     ...           A121  33   A141    A152       2   \n",
       "4            A92       A101     ...           A123  34   A143    A152       1   \n",
       "\n",
       "    job maintenance  telephone foreigner creditworthy  \n",
       "0  A173           1       A191       NaN            1  \n",
       "1   NaN           1       A191      A201            1  \n",
       "2   NaN           2       A191       NaN            2  \n",
       "3  A173           1       A191      A201            1  \n",
       "4  A173           2       A192      A201            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=get_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prüfen der Daten und Columnnames\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def understand_data():\n",
    "\n",
    "    #Mittelwerte, Standardabweichung, Maxima\n",
    "    df=get_data()\n",
    "    np.mean(df)\n",
    "    np.std(df)\n",
    "    np.max(df)\n",
    "\n",
    "    #Ausgabe der einzelnen Werte pro Kategorie für jedes Attribut\n",
    "    for feature in df.columns:\n",
    "        print df[feature].value_counts() \n",
    "        \n",
    "    #Anzahl an Attributen und Beispielen\n",
    "    print df.shape\n",
    "    \n",
    "    #Anteil an Kreditwuerdigkeit und Nicht-Kreditwuerdigkeit\n",
    "    print \"Kreditwuerdig\"\n",
    "    print sum(df['creditworthy']==1)\n",
    "    print \"Nicht-Kreditwuerdig\"\n",
    "    print sum(df['creditworthy']==2)\n",
    "       \n",
    "    numeric=['duration','amount', 'rate', 'residence', 'age', 'credits', 'maintenance']\n",
    "    zeroone=['telephone', 'creditworthy']\n",
    "    missing=['purpose', 'employment', 'job', 'foreigner']\n",
    "    \n",
    "    df_nonum=df[df.columns.difference(numeric+zeroone)]\n",
    "    df_num=df[numeric]\n",
    "\n",
    "    #Anzahl der fehlenden Werte und Zusammenhang des Attributes mit Zielattribut\n",
    "    for feature in df[missing]:\n",
    "        print df[feature].value_counts()  \n",
    "        print \"Missing\"\n",
    "        print sum((df.ix[(df[feature].isnull()==True),'creditworthy'])==1)\n",
    "    \n",
    "    #Kategoriale Werte hinsichtlich Kreditwuerdigkeit geordnet\n",
    "    for feature in df_nonum.columns: \n",
    "        plt.figure()\n",
    "        df_group= df.groupby(['creditworthy'])\n",
    "        freq=df_group[feature].value_counts()\n",
    "        (freq.ix[0]/freq.ix[1]).plot(kind='bar')\n",
    "        plt.title(feature)\n",
    "        (freq.ix[0]/freq.ix[1]).plot(kind='line', color='red')\n",
    "        plt.show()\n",
    "    \n",
    "    #Verteilungen der nummerischen Werte, getrennt nach Kreditwuerdigkeit    \n",
    "    for feature in df_num.columns: \n",
    "        df.boxplot(feature, by='creditworthy')\n",
    "        plt.show()\n",
    "        \n",
    "    #Notwendigkeit und Effekt der Z-Standardisierung      \n",
    "    df_num.boxplot()\n",
    "    plt.show()\n",
    "    zstand=pd.DataFrame(preprocessing.scale(df_num))\n",
    "    zstand.columns=numeric\n",
    "    zstand.boxplot()\n",
    "    plt.show()\n",
    "    \n",
    "    #Range der nummerischen Feature\n",
    "    for feature in df_num.columns: \n",
    "        print feature, \":\",  max(df_num[feature]), (df_num[feature])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#understand_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of categorical values for linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def get_lin(stand=0):\n",
    "    \n",
    "    #Daten abrufen und unterteilen nach nummerisch, fehlend, nicht-nummerisch\n",
    "    df=get_data()\n",
    "    X=df.ix[:,df.columns!='creditworthy']\n",
    "    y=(df.creditworthy ==1)*1  # 1=yes and 0=no\n",
    "\n",
    "    numeric=['duration','amount', 'rate', 'residence', 'age', 'credits', 'maintenance']\n",
    "    X_num=X[numeric]\n",
    "    missing=['purpose', 'employment', 'job', 'foreigner']\n",
    "    \n",
    "    #z-Standardisierung\n",
    "    X_num_scaled =pd.DataFrame(preprocessing.scale(X_num))\n",
    "    X_num_scaled.columns=numeric\n",
    "    #print X_num_scaled.head()\n",
    "\n",
    "    #Erzeugen von Dummyvariablen aus kategorialen vollstaendigen Variablen\n",
    "    X_nonum=X[X.columns.difference(numeric+missing)]\n",
    "    X_dum = pd.DataFrame()\n",
    "    for feature in X_nonum.columns: \n",
    "        dummies=pd.get_dummies(X_nonum[feature]) \n",
    "        X_dum=pd.concat([X_dum, dummies], axis=1)\n",
    "\n",
    "    #Frame fuer Lineare Modelle erstellen (wenn stand==1 werden z-stand. Werte genutzt)\n",
    "    X_lin=pd.concat([X[numeric], X_dum], axis=1)\n",
    "    if stand==1:\n",
    "        X_lin=pd.concat([X_num_scaled, X_dum], axis=1)\n",
    "    X_miss=X[missing]\n",
    "    \n",
    "    return X_miss, X_lin, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'purpose', u'employment', u'job', u'foreigner'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_miss, X_lin, y = get_lin()\n",
    "print X_miss.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values (Linear Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from scipy.stats import mode\n",
    "\n",
    "def get_full(stand=0, mode=0):\n",
    "    \n",
    "    df=get_data()    \n",
    "    X=pd.DataFrame(df.ix[:,df.columns!='creditworthy']) #Ursprungsframe\n",
    "    X_miss, X_lin, y=get_lin(stand=stand) #vorverarbeites Frame (dummykodiert und evtl. z-stand.)\n",
    "\n",
    "    missing=['purpose', 'employment', 'job', 'foreigner']\n",
    "    X_wasmiss=pd.DataFrame()\n",
    "    X_filled=pd.DataFrame()\n",
    "    \n",
    "    #Alle Attribute mit fehlenden Werten werden nacheinander imputiert\n",
    "    \n",
    "    for feature in missing: \n",
    "    \n",
    "        #nur vorhandene Werte zum Training nehmen\n",
    "        NaNs=(X_miss[feature].isnull()==True)\n",
    "        NoNaNs=(X_miss[feature].isnull()==False)\n",
    "        X_nomiss=X_lin[NoNaNs] \n",
    "        y_nomiss=X_miss.ix[NoNaNs, feature]\n",
    "        \n",
    "        #Labelencoding des Missing-Features         \n",
    "        le = preprocessing.LabelEncoder() \n",
    "        le.fit(np.unique(y_nomiss))\n",
    "        y_nomiss=le.transform(y_nomiss)\n",
    "\n",
    "        #Train-Test-Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_nomiss, y_nomiss, random_state=42)  \n",
    "\n",
    "        #Logistic Regression (Linear Classification)\n",
    "        tuned_lm_parameters = [{'C':[0.01, 0.05, 0.1, 0.5, 1.0, 1.1, 1.5, 2.0], 'penalty': ['l1', 'l2']}]      \n",
    "        lr=GridSearchCV(LogisticRegression(random_state=42),tuned_lm_parameters, \n",
    "                        cv=StratifiedKFold(y_train, 5, random_state=42))        \n",
    "        lr.fit(X_train, y_train) # Trainieren, um Modell zu prüfen\n",
    "\n",
    "        if mode==0:\n",
    "            print feature, ':', lr.score(X_test, y_test) # Ausgabe der Accuracy\n",
    "            lr.fit(X_nomiss, y_nomiss) #Training auf allen Daten\n",
    "            pred= lr.predict(X_lin[NaNs]) #Missing-Value-Prediction \n",
    "            X_miss.ix[NaNs,feature]=list(le.inverse_transform(pred)) #diese Rueckkodieren & einfuegen\n",
    "        \n",
    "        #alternative Methode: Modus\n",
    "        if mode==1:\n",
    "            correct=0\n",
    "            modus=le.transform(X[feature].value_counts().argmax())\n",
    "            for i in range(len(y_test)):\n",
    "                if (y_test[i]== modus):\n",
    "                    correct=correct+1\n",
    "            print feature, ':', (1.*correct)/len(y_test)\n",
    "            X_miss.ix[NaNs, feature]=X[feature].value_counts().argmax() #diese Rueckkodieren & einfuegen\n",
    "\n",
    "        #alternative Methode: behalten der fehlenden Werte\n",
    "        if mode==2:\n",
    "            print 'es wurden keine fehlenden Werte ersetzt'\n",
    "            \n",
    "        #Kodieren, welche vorher NaNs waren\n",
    "        X_wasmiss=pd.concat([X_wasmiss, NaNs], axis=1)\n",
    "    \n",
    "    X_wasmiss.columns=['purpose_nan', 'employment_nan', 'job_nan', 'foreigner_nan']\n",
    "      \n",
    "    #Erzeugen eines Dummykodierten-Frames\n",
    "    numeric=['duration','amount', 'rate', 'residence', 'age', 'credits', 'maintenance']\n",
    "    X_nonum=df[X.columns.difference(numeric)]\n",
    "    X_dum = pd.DataFrame()\n",
    "    for feature in X_nonum.columns: \n",
    "        dummies=pd.get_dummies(X_nonum[feature]) \n",
    "        X_dum=pd.concat([X_dum, dummies], axis=1)\n",
    "        \n",
    "    X_lin=pd.concat([X[numeric], X_dum, X_wasmiss, pd.get_dummies(X_miss)], axis=1)\n",
    "      \n",
    "    y=(df.creditworthy ==1)*1  # 1=yes and 0=no\n",
    "                     \n",
    "    return X, X_lin, y;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Werte wurden durch lineare Classification ersetzt\n",
      "purpose : 0.270531400966\n",
      "employment : 0.436507936508\n",
      "job : 0.635416666667\n",
      "foreigner : 0.94375\n",
      "mit z-Standaridisierung\n",
      "purpose : 0.280193236715\n",
      "employment : 0.47619047619\n",
      "job : 0.645833333333\n",
      "foreigner : 0.94375\n",
      "Zum Vergleich: Vorhersage durch Modus\n",
      "purpose : 0.246376811594\n",
      "employment : 0.31746031746\n",
      "job : 0.625\n",
      "foreigner : 0.94375\n"
     ]
    }
   ],
   "source": [
    "print \"Fehlende Werte wurden durch lineare Classification ersetzt\" \n",
    "X, X_lin, y =  get_full(stand=0, mode=0)\n",
    "print \"mit z-Standaridisierung\"\n",
    "X, X_lin, y =  get_full(stand=1, mode=0)\n",
    "print \"Zum Vergleich: Vorhersage durch Modus\" \n",
    "X, X_lin, y =  get_full(stand=0, mode=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss-Function, die berücksichtigt, dass fp fünfmal so schlimm wie fn\n",
    "def weighted_loss(y_test, y_pred):\n",
    "    y_test = np.array(y_test)\n",
    "    fp=np.sum(np.logical_and(y_pred!=y_test, y_pred==1))\n",
    "    fn=np.sum(np.logical_and(y_pred!=y_test, y_pred==0))\n",
    "    costs=1.*(fp*5 + fn)/len(y_test) #eventuell noch durch sechs teilen\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation mit Kostenmodel (5xFP==FN) und Baseline (immer ja/nein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def base(y_test):\n",
    "    print \"Baseline: Immer kreditwürdig\"\n",
    "    evaluate(np.ones((len(y_test))), y_test)\n",
    "    print \"Baseline: Immer nicht-kreditwürdig\"\n",
    "    evaluate(np.zeros(len(y_test)), y_test)\n",
    "    \n",
    "def evaluate(y_pred, y_test):\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    acc=1.*(np.sum(y_pred==y_test))/len(y_test)\n",
    "    classif_rate = np.mean(y_pred == y_test) * 100\n",
    "    \n",
    "    false=1.*(np.sum(y_pred!=y_test))/len(y_test)\n",
    "    \n",
    "    fp=np.sum(np.logical_and(y_pred!=y_test, y_pred==1))\n",
    "    fn=np.sum(np.logical_and(y_pred!=y_test, y_pred==0))\n",
    "    \n",
    "    costs=1.*(fp*5 + fn)/len(y_test) \n",
    "    \n",
    "    print \"Accuracy: {}\".format(acc) \n",
    "    print \"False Positives: {}\".format(fp) \n",
    "    print \"False Negatives: {}\".format(fn) \n",
    "    print \"Kosten: {}\".format(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Crossvalidation (nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def prediction(stand, mode):\n",
    "    \n",
    "# Loading the dataset\n",
    "    print \"Alle nummerischen Attribute wurden z-standardisiert.\"\n",
    "    print \"Fehlende Werte wurden durch lineare Classification ersetzt (Accuracy):\" \n",
    "    X, X_lin, y =  get_full(stand, mode) \n",
    "\n",
    "    #Initialisierung verschiedener Variablen\n",
    "    y_true = []\n",
    "    y_pred_lr = []\n",
    "    y_pred_rf = []\n",
    "    y_pred_svm = []\n",
    "    mean_tpr_lr = np.zeros((3, 300))\n",
    "    mean_fpr_lr = np.zeros((3, 300))\n",
    "    mean_tpr_rf = np.zeros((3, 300))\n",
    "    mean_fpr_rf = np.zeros((3, 300))\n",
    "    mean_tpr_svm = np.zeros((3, 300))\n",
    "    mean_fpr_svm = np.zeros((3, 300))\n",
    "    i=0\n",
    "\n",
    "    #Festlegen der mit Crossvalidierung zu tunenden Parameter\n",
    "    tuned_rt_parameters = [{'max_depth':[4,5,6,7,8,9,10],\n",
    "                           'min_samples_leaf':[5,6,7,8,9,10,11,12],                        \n",
    "                           'criterion':['entropy', 'gini']}]\n",
    "\n",
    "    tuned_lm_parameters = [{'C':[0.05, 0.5],#, 1.0, 1.5, 2.0], \n",
    "                          }]\n",
    "\n",
    "    score=make_scorer(weighted_loss, greater_is_better=False)\n",
    "    plt.figure(1)\n",
    "\n",
    "    #Nested Cross-Validation\n",
    "    #Stratifizierung -> 30:70 Verhaeltnis bleibt erhalten fuer Test und Train\n",
    "    for train, test in StratifiedShuffleSplit(y, n_iter=3, test_size=0.3, random_state=42):\n",
    "        y_true = np.append(y_true, y[test])\n",
    "\n",
    "        #Random-Forest Classifier mit getunten Hyperparametern\n",
    "        rf_clf = GridSearchCV(RandomForestClassifier(n_estimators=1000, class_weight={1: 1, 0:5}, \n",
    "                                                    random_state=42), \n",
    "                           tuned_rt_parameters, cv=StratifiedKFold(y[train], 5), scoring=score)\n",
    "        rf_clf.fit(X_lin.ix[train], y[train])\n",
    "        y_pred_rf = np.append(y_pred_rf, rf_clf.predict(X_lin.ix[test]))\n",
    "\n",
    "        #Logistic Regression Classifier mit getunten Hyperparametern\n",
    "        clf = GridSearchCV(LogisticRegression(class_weight={1: 1, 0:5}, random_state=42),\n",
    "                           tuned_lm_parameters, cv=StratifiedKFold(y[train], 5), scoring=score)\n",
    "        clf.fit(X_lin.ix[train], y[train])\n",
    "        y_pred_lr = np.append(y_pred_lr, clf.predict(X_lin.ix[test]))\n",
    "\n",
    "        #Evaluation der Classifier auf den Training-Test-Splits\n",
    "        print(\"Beste Parameter fuer Random Forest auf einem der Trainingssets:\")\n",
    "        print(rf_clf.best_estimator_)\n",
    "        \n",
    "        print(\"Werte für den RF auf einem der Trainingssets:\")\n",
    "        evaluate(rf_clf.predict(X_lin.ix[test]), y[test])\n",
    "        print metrics.classification_report(y[test],rf_clf.predict(X_lin.ix[test]))\n",
    "\n",
    "        print(\"Beste Parameter fuer Logistic Regression  auf einem der Trainingssets:\")\n",
    "        print(clf.best_estimator_)\n",
    "\n",
    "        print(\"Werte für die LR auf einem der Trainingssets:\")\n",
    "        evaluate(clf.predict(X_lin.ix[test]), y[test])\n",
    "        print metrics.classification_report(y[test],clf.predict(X_lin.ix[test]))\n",
    "\n",
    "        #ROC-Kurven\n",
    "        \n",
    "        plt.subplot(211)\n",
    "        #ROC-KURVE fuer Random Forrest\n",
    "        probs_rf = rf_clf.fit(X_lin.ix[train], y[train]).predict_proba(X_lin.ix[test])\n",
    "        fpr_rf, tpr_rf, thresholds_rf = roc_curve(y[test], probs_rf[:, 1], drop_intermediate=False)\n",
    "        roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "        plt.plot(fpr_rf, tpr_rf, lw=1, label='RandFor: ROC (area = %0.2f)' % (roc_auc_rf))\n",
    "\n",
    "        mean_fpr_rf[i,:]=fpr_rf\n",
    "        mean_tpr_rf[i,:]=tpr_rf\n",
    "\n",
    "        #ROC-KURVE fuer logistisch Regression\n",
    "        probs_lr = clf.fit(X_lin.ix[train], y[train]).predict_proba(X_lin.ix[test])\n",
    "        fpr_lr, tpr_lr, thresholds_lr = roc_curve(y[test], probs_lr[:, 1], drop_intermediate=False)\n",
    "        roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "        plt.plot(fpr_lr, tpr_lr, lw=1, label='LogReg: ROC (area = %0.2f)' % (roc_auc_lr))\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        mean_fpr_lr[i,:]=fpr_lr\n",
    "        mean_tpr_lr[i,:]=tpr_lr\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    #Gemittelte ROC-Kurven\n",
    "    plt.subplot(212)\n",
    "\n",
    "    #Gemittelte ROC-KURVE fuer RF\n",
    "    mean_fpr_rf= np.mean(mean_fpr_rf, axis=0)\n",
    "    mean_tpr_rf= np.mean(mean_tpr_rf, axis=0)\n",
    "    mean_auc_rf = auc(mean_fpr_rf, mean_tpr_rf)\n",
    "    plt.plot(mean_fpr_rf, mean_tpr_rf, 'r',\n",
    "             label='RF: Mean ROC (area = %0.2f)' % mean_auc_rf, lw=2)\n",
    "\n",
    "    #Gemittelte ROC-KURVE fuer LR\n",
    "    mean_fpr_lr= np.mean(mean_fpr_lr, axis=0)\n",
    "    mean_tpr_lr= np.mean(mean_tpr_lr, axis=0)\n",
    "    mean_auc_lr = auc(mean_fpr_lr, mean_tpr_lr)\n",
    "    plt.plot(mean_fpr_lr, mean_tpr_lr, 'b',\n",
    "             label='LR: Mean ROC (area = %0.2f)' % mean_auc_lr, lw=2)\n",
    "    \n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    #Evaluation der crossvalidierten Ergebnisse \n",
    "\n",
    "    print 'Crossvalidierte Ergebnisse fuer Random Forest'\n",
    "    evaluate(y_pred_rf, y_true)\n",
    "    print metrics.classification_report(y_true,y_pred_lr)\n",
    "\n",
    "    print 'Crossvalidierte Ergebnisse fuer Logistic Regression'\n",
    "    evaluate(y_pred_lr, y_true)\n",
    "    print metrics.classification_report(y_true,y_pred_lr)\n",
    "\n",
    "    #Zum Vergleich: Ergebnisse der Baseline\n",
    "    print 'Ergebnisse der Baseline'\n",
    "    base(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell mit Standardisierung\n",
      "Alle nummerischen Attribute wurden z-standardisiert.\n",
      "Fehlende Werte wurden durch lineare Classification ersetzt (Accuracy):\n",
      "purpose : 0.280193236715\n",
      "employment : 0.47619047619\n",
      "job : 0.645833333333\n",
      "foreigner : 0.94375\n",
      "Beste Parameter fuer Random Forest auf einem der Trainingssets:\n",
      "RandomForestClassifier(bootstrap=True, class_weight={0: 5, 1: 1},\n",
      "            criterion='entropy', max_depth=5, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Werte für den RF auf einem der Trainingssets:\n",
      "Accuracy: 0.563333333333\n",
      "False Positives: 6\n",
      "False Negatives: 125\n",
      "Kosten: 0.516666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.93      0.56        90\n",
      "          1       0.93      0.40      0.56       210\n",
      "\n",
      "avg / total       0.77      0.56      0.56       300\n",
      "\n",
      "Beste Parameter fuer Logistic Regression  auf einem der Trainingssets:\n",
      "LogisticRegression(C=0.05, class_weight={0: 5, 1: 1}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Werte für die LR auf einem der Trainingssets:\n",
      "Accuracy: 0.59\n",
      "False Positives: 10\n",
      "False Negatives: 113\n",
      "Kosten: 0.543333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.89      0.57        90\n",
      "          1       0.91      0.46      0.61       210\n",
      "\n",
      "avg / total       0.76      0.59      0.60       300\n",
      "\n",
      "Beste Parameter fuer Random Forest auf einem der Trainingssets:\n",
      "RandomForestClassifier(bootstrap=True, class_weight={0: 5, 1: 1},\n",
      "            criterion='entropy', max_depth=5, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Werte für den RF auf einem der Trainingssets:\n",
      "Accuracy: 0.566666666667\n",
      "False Positives: 5\n",
      "False Negatives: 125\n",
      "Kosten: 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.94      0.57        90\n",
      "          1       0.94      0.40      0.57       210\n",
      "\n",
      "avg / total       0.78      0.57      0.57       300\n",
      "\n",
      "Beste Parameter fuer Logistic Regression  auf einem der Trainingssets:\n",
      "LogisticRegression(C=0.05, class_weight={0: 5, 1: 1}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Werte für die LR auf einem der Trainingssets:\n",
      "Accuracy: 0.646666666667\n",
      "False Positives: 12\n",
      "False Negatives: 94\n",
      "Kosten: 0.513333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.87      0.60        90\n",
      "          1       0.91      0.55      0.69       210\n",
      "\n",
      "avg / total       0.77      0.65      0.66       300\n",
      "\n",
      "Beste Parameter fuer Random Forest auf einem der Trainingssets:\n",
      "RandomForestClassifier(bootstrap=True, class_weight={0: 5, 1: 1},\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Werte für den RF auf einem der Trainingssets:\n",
      "Accuracy: 0.643333333333\n",
      "False Positives: 14\n",
      "False Negatives: 93\n",
      "Kosten: 0.543333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.84      0.59        90\n",
      "          1       0.89      0.56      0.69       210\n",
      "\n",
      "avg / total       0.76      0.64      0.66       300\n",
      "\n",
      "Beste Parameter fuer Logistic Regression  auf einem der Trainingssets:\n",
      "LogisticRegression(C=0.5, class_weight={0: 5, 1: 1}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Werte für die LR auf einem der Trainingssets:\n",
      "Accuracy: 0.68\n",
      "False Positives: 14\n",
      "False Negatives: 82\n",
      "Kosten: 0.506666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.84      0.61        90\n",
      "          1       0.90      0.61      0.73       210\n",
      "\n",
      "avg / total       0.78      0.68      0.69       300\n",
      "\n",
      "Crossvalidierte Ergebnisse fuer Random Forest\n",
      "Accuracy: 0.591111111111\n",
      "False Positives: 25\n",
      "False Negatives: 343\n",
      "Kosten: 0.52\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.87      0.59       270\n",
      "        1.0       0.90      0.54      0.68       630\n",
      "\n",
      "avg / total       0.77      0.64      0.65       900\n",
      "\n",
      "Crossvalidierte Ergebnisse fuer Logistic Regression\n",
      "Accuracy: 0.638888888889\n",
      "False Positives: 36\n",
      "False Negatives: 289\n",
      "Kosten: 0.521111111111\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.87      0.59       270\n",
      "        1.0       0.90      0.54      0.68       630\n",
      "\n",
      "avg / total       0.77      0.64      0.65       900\n",
      "\n",
      "Ergebnisse der Baseline\n",
      "Baseline: Immer kreditwürdig\n",
      "Accuracy: 0.7\n",
      "False Positives: 270\n",
      "False Negatives: 0\n",
      "Kosten: 1.5\n",
      "Baseline: Immer nicht-kreditwürdig\n",
      "Accuracy: 0.3\n",
      "False Positives: 0\n",
      "False Negatives: 630\n",
      "Kosten: 0.7\n"
     ]
    }
   ],
   "source": [
    "print 'Modell mit Standardisierung'\n",
    "prediction(stand=1, mode=0)\n",
    "\n",
    "\n",
    "#print 'Modell ohne Standardisierung'\n",
    "#prediction(stand=0, mode=0)\n",
    "#print 'Modell ohne Imputation'\n",
    "#prediction(stand=1, mode=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechung des Finalen Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purpose : 0.270531400966\n",
      "employment : 0.436507936508\n",
      "job : 0.635416666667\n",
      "foreigner : 0.94375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 5, 1: 1},\n",
       "            criterion='entropy', max_depth=6, max_features='auto',\n",
       "            max_leaf_nodes=None, min_samples_leaf=7, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Loading the dataset\n",
    "X, X_lin, y = get_full()\n",
    "\n",
    "#Trainieren des finalen Modells auf allen Daten: \n",
    "\n",
    "final_clf = RandomForestClassifier(class_weight={0: 5, 1: 1}, criterion='entropy', max_depth=6, \n",
    "                                   min_samples_leaf=7, n_estimators=1000, random_state=42)\n",
    "         \n",
    "final_clf.fit(X_lin, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
